{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.optim import Optimizer\n",
    "import os, math \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxFactor(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=0.01, beta2_decay=-0.8, eps=(1e-12, 1e-8), d=1.0, \n",
    "                 weight_decay=0.25, gamma=0.99, max=False,\n",
    "                 full_matrix=False, clip=0.95):\n",
    "                   \n",
    "        eps1, eps2 = eps\n",
    "        if eps1 is None:\n",
    "            eps1 = torch.finfo(torch.float32).eps\n",
    "            \n",
    "        defaults = dict(\n",
    "            lr=lr, beta2_decay=beta2_decay, eps=(eps1, eps2), d=d,  weight_decay=weight_decay, \n",
    "            gamma=gamma, max=max, full_matrix=full_matrix, clip=clip)\n",
    "        \n",
    "        super().__init__(params=params, defaults=defaults)\n",
    "        \n",
    "    def _get_lr(self, param_group, param_state):\n",
    "            step = param_state[\"step\"]\n",
    "            step_float = step.item()\n",
    "            decay_factor = min(1.0, 1.0 / (step_float ** 0.4  + 1e-12))\n",
    "            param_scale = max(param_group[\"eps\"][1], param_state[\"RMS\"])\n",
    "            return min(param_group[\"lr\"], param_scale * decay_factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rms(tensor):\n",
    "        if tensor.numel() == 0:\n",
    "            return torch.tensor(0.0, device=tensor.device)\n",
    "        return tensor.norm() / (tensor.numel() ** 0.5 + 1e-12)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            row_vars = []\n",
    "            col_vars = []\n",
    "            v = []\n",
    "            state_steps = []\n",
    "            eps1, eps2 = group[\"eps\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "                if grad.dtype in {torch.float16, torch.bfloat16}:\n",
    "                    grad = grad.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = torch.tensor(0.0, dtype=torch.float32)\n",
    "                    \n",
    "                    if p.dim() > 1 and not group[\"full_matrix\"]:\n",
    "                        row_shape = list(p.shape)\n",
    "                        row_shape[-1] = 1\n",
    "                        state[\"row_var\"] = torch.zeros(row_shape, dtype=torch.float32, device=p.device)\n",
    "                        \n",
    "                        col_shape = list(p.shape)\n",
    "                        col_shape[-2] = 1\n",
    "                        state[\"col_var\"] = torch.zeros(col_shape, dtype=torch.float32, device=p.device)\n",
    "                    \n",
    "                    state[\"v\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    \n",
    "                    state[\"RMS\"] = self._rms(p).item()\n",
    "\n",
    "                row_vars.append(state.get(\"row_var\", None))\n",
    "                col_vars.append(state.get(\"col_var\", None))\n",
    "                v.append(state[\"v\"])\n",
    "                state_steps.append(state[\"step\"])\n",
    "                params_with_grad.append(p)\n",
    "                grads.append(grad)\n",
    "\n",
    "            for i, param in enumerate(params_with_grad):\n",
    "                grad = grads[i]\n",
    "                state = self.state[param]\n",
    "                                \n",
    "                if group[\"max\"]:\n",
    "                    grad = -grad\n",
    "                    \n",
    "                step_t = state_steps[i]\n",
    "                row_var = row_vars[i]\n",
    "                col_var = col_vars[i]\n",
    "                vi = v[i]\n",
    "                \n",
    "                step_t += 1\n",
    "                step_float = step_t.item()\n",
    "                \n",
    "                one_minus_beta2_t = min(0.999, step_float ** group[\"beta2_decay\"])\n",
    "\n",
    "                state = self.state[param]\n",
    "                state[\"RMS\"] = self._rms(param).item()\n",
    "                adaptive_lr = self._get_lr(group, state)\n",
    "                \n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    param.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] + eps1)\n",
    "\n",
    "                if param.dim() > 1 and not group[\"full_matrix\"]:\n",
    "                    row_mean = torch.norm(grad, dim=-1, keepdim=True).square_()\n",
    "                    row_mean.div_(grad.size(-1) + eps1)\n",
    "                    row_var.lerp_(row_mean, one_minus_beta2_t)\n",
    "                    \n",
    "                    col_mean = torch.norm(grad, dim=-2, keepdim=True).square_()\n",
    "                    col_mean.div_(grad.size(-2) + eps1)\n",
    "                    col_var.lerp_(col_mean, one_minus_beta2_t)\n",
    "                    \n",
    "                    var_estimate = row_var @ col_var\n",
    "                    max_row_var = row_var.max(dim=-2, keepdim=True)[0]  \n",
    "                    var_estimate.div_(max_row_var.clamp_(min=eps1))\n",
    "                else:\n",
    " \n",
    "                    vi.mul_(group[\"gamma\"]).add_(grad.square_(), alpha=1 - group[\"gamma\"])\n",
    "                    var_estimate = vi\n",
    "                    \n",
    "                update = var_estimate.clamp_(min=eps1 * eps1).rsqrt_().mul_(grad)\n",
    "                \n",
    "                inf_norm = torch.norm(update, float('inf'))\n",
    "                if inf_norm > 0:\n",
    "                    update.div_(inf_norm.clamp_(min=eps1))\n",
    "                \n",
    "                if group.get(\"clip\", 0) > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        [update], \n",
    "                        max_norm=group[\"clip\"]\n",
    "                    )\n",
    "                \n",
    "                l2_norm = update.norm(2).item()\n",
    "                denom = max(1.0, l2_norm / ((update.numel() ** 0.5) * group[\"d\"]))\n",
    "                \n",
    "                if param.dim() > 1:\n",
    "                    param.add_(\n",
    "                        update.sign() * update.abs().max(dim=-1, keepdim=True)[0], \n",
    "                        alpha=-adaptive_lr / denom\n",
    "                    )\n",
    "                else:\n",
    "                    param.add_(update, alpha=-adaptive_lr / denom)\n",
    "     \n",
    "                state[\"step\"] = step_t\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def optimizer_benchmark(model_fn, dataset_name='mnist', batch_size=128, \n",
    "                        epochs=10, learning_rates=None, seeds=None,\n",
    "                        optimizers_dict=None, save_plots=True):\n",
    "    \"\"\"\n",
    "    Benchmark different optimizers on a given model and dataset.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function that returns a model\n",
    "        dataset_name: Name of dataset to use ('mnist', 'cifar10')\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of epochs to train\n",
    "        learning_rates: Dictionary mapping optimizer names to learning rates\n",
    "        seeds: List of random seeds to use\n",
    "        optimizers_dict: Dictionary mapping names to optimizer constructors\n",
    "        save_plots: Whether to save plots to files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    \"\"\"\n",
    "    # Default parameters\n",
    "    if seeds is None:\n",
    "        seeds = [42, 123, 456]  # Multiple seeds for statistical significance\n",
    "        \n",
    "    if learning_rates is None:\n",
    "        learning_rates = {\n",
    "            'SGD': 0.01,\n",
    "            'Adam': 0.001,\n",
    "            'AdamW': 0.001,\n",
    "            'MaxFactor': 0.025\n",
    "        }\n",
    "    \n",
    "    if optimizers_dict is None:\n",
    "        optimizers_dict = {\n",
    "            'SGD': lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9),\n",
    "            'Adam': lambda params, lr: torch.optim.Adam(params, lr=lr),\n",
    "            'AdamW': lambda params, lr: torch.optim.AdamW(params, lr=lr, weight_decay=0.0025),\n",
    "            'MaxFactor': lambda params, lr: MaxFactor(params=params, lr=lr)\n",
    "\n",
    "        }\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if dataset_name.lower() == 'mnist':\n",
    "        train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                      ]))\n",
    "        test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                                     transform=transforms.Compose([\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                     ]))\n",
    "        input_channels = 1\n",
    "        num_classes = 10\n",
    "    elif dataset_name.lower() == 'cifar10':\n",
    "        train_dataset = datasets.CIFAR10('./data', train=True, download=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.RandomCrop(32, padding=4),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                                                                (0.2023, 0.1994, 0.2010))\n",
    "                                        ]))\n",
    "        test_dataset = datasets.CIFAR10('./data', train=False, download=True,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                                                               (0.2023, 0.1994, 0.2010))\n",
    "                                       ]))\n",
    "        input_channels = 3\n",
    "        num_classes = 10\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "    \n",
    "    # For quicker iterations, use a subset of the data during development\n",
    "    # Remove this for full benchmarks\n",
    "    train_dataset = Subset(train_dataset, range(5000))  # 5000 samples for training\n",
    "    test_dataset = Subset(test_dataset, range(1000))    # 1000 samples for testing\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        'train_loss': {},\n",
    "        'train_acc': {},\n",
    "        'test_loss': {},\n",
    "        'test_acc': {},\n",
    "        'time_per_epoch': {},\n",
    "        'param_update_norm': {},\n",
    "        'grad_norm': {},\n",
    "    }\n",
    "    \n",
    "    # Initialize results dictionaries\n",
    "    for opt_name in optimizers_dict.keys():\n",
    "        results['train_loss'][opt_name] = []\n",
    "        results['train_acc'][opt_name] = []\n",
    "        results['test_loss'][opt_name] = []\n",
    "        results['test_acc'][opt_name] = []\n",
    "        results['time_per_epoch'][opt_name] = []\n",
    "        results['param_update_norm'][opt_name] = []\n",
    "        results['grad_norm'][opt_name] = []\n",
    "    \n",
    "    # Run benchmark for each optimizer\n",
    "    for opt_name, opt_constructor in optimizers_dict.items():\n",
    "        print(f\"\\nBenchmarking optimizer: {opt_name}\")\n",
    "        \n",
    "        # Store results across seeds\n",
    "        seed_train_losses = []\n",
    "        seed_train_accs = []\n",
    "        seed_test_losses = []\n",
    "        seed_test_accs = []\n",
    "        seed_times = []\n",
    "        seed_update_norms = []\n",
    "        seed_grad_norms = []\n",
    "        \n",
    "        for seed_idx, seed in enumerate(seeds):\n",
    "            print(f\"  Running with seed {seed} ({seed_idx+1}/{len(seeds)})\")\n",
    "            set_seed(seed)\n",
    "            \n",
    "            # Create model\n",
    "            model = model_fn(input_channels, num_classes).to(device)\n",
    "            \n",
    "            # Create optimizer\n",
    "            lr = learning_rates.get(opt_name, 0.01)\n",
    "            optimizer = opt_constructor(model.parameters(), lr)\n",
    "            \n",
    "            # Create scheduler (cosine annealing)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=epochs, eta_min=lr/10\n",
    "            )\n",
    "            \n",
    "            # Lists to store metrics for this run\n",
    "            train_losses = []\n",
    "            train_accs = []\n",
    "            test_losses = []\n",
    "            test_accs = []\n",
    "            epoch_times = []\n",
    "            update_norms = []\n",
    "            grad_norms = []\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(epochs):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Train\n",
    "                model.train()\n",
    "                train_loss, train_acc, epoch_update_norm, epoch_grad_norm = train_epoch(\n",
    "                    model, train_loader, optimizer, device\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                model.eval()\n",
    "                test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "                \n",
    "                # Step scheduler\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Record time\n",
    "                epoch_time = time.time() - start_time\n",
    "                \n",
    "                # Store metrics\n",
    "                train_losses.append(train_loss)\n",
    "                train_accs.append(train_acc)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accs.append(test_acc)\n",
    "                epoch_times.append(epoch_time)\n",
    "                update_norms.append(epoch_update_norm)\n",
    "                grad_norms.append(epoch_grad_norm)\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"    Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "                      f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, \"\n",
    "                      f\"Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Store results for this seed\n",
    "            seed_train_losses.append(train_losses)\n",
    "            seed_train_accs.append(train_accs)\n",
    "            seed_test_losses.append(test_losses)\n",
    "            seed_test_accs.append(test_accs)\n",
    "            seed_times.append(epoch_times)\n",
    "            seed_update_norms.append(update_norms)\n",
    "            seed_grad_norms.append(grad_norms)\n",
    "        \n",
    "        # Average results across seeds\n",
    "        results['train_loss'][opt_name] = np.mean(seed_train_losses, axis=0)\n",
    "        results['train_acc'][opt_name] = np.mean(seed_train_accs, axis=0)\n",
    "        results['test_loss'][opt_name] = np.mean(seed_test_losses, axis=0)\n",
    "        results['test_acc'][opt_name] = np.mean(seed_test_accs, axis=0)\n",
    "        results['time_per_epoch'][opt_name] = np.mean(seed_times, axis=0)\n",
    "        results['param_update_norm'][opt_name] = np.mean(seed_update_norms, axis=0)\n",
    "        results['grad_norm'][opt_name] = np.mean(seed_grad_norms, axis=0)\n",
    "    \n",
    "    # Plot results\n",
    "    if save_plots:\n",
    "        plot_metrics(results, dataset_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train model for one epoch and return metrics\"\"\"\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For tracking parameter updates and gradient norms\n",
    "    param_update_norm = 0\n",
    "    grad_norm = 0\n",
    "    update_samples = 0\n",
    "    \n",
    "    # Store initial parameters for computing updates\n",
    "    initial_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        initial_params[name] = param.clone().detach()\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calculate gradient norm before optimizer step\n",
    "        batch_grad_norm = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                batch_grad_norm += param.grad.norm(2).item() ** 2\n",
    "        grad_norm += batch_grad_norm ** 0.5\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Sample parameter updates to avoid excessive memory usage\n",
    "        if batch_idx % 5 == 0:  # Sample every 5 batches\n",
    "            batch_update_norm = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in initial_params:\n",
    "                    update = param.detach() - initial_params[name]\n",
    "                    batch_update_norm += update.norm(2).item() ** 2\n",
    "                    # Update initial params for next comparison\n",
    "                    initial_params[name] = param.clone().detach()\n",
    "            param_update_norm += batch_update_norm ** 0.5\n",
    "            update_samples += 1\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    # Normalize metrics\n",
    "    train_loss /= len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    param_update_norm = param_update_norm / max(1, update_samples)\n",
    "    grad_norm /= len(train_loader)\n",
    "    \n",
    "    return train_loss, accuracy, param_update_norm, grad_norm\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test data\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return test_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot_metrics(results, dataset_name):\n",
    "    \"\"\"Plot benchmark metrics\"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    for opt_name, losses in results['train_loss'].items():\n",
    "        plt.plot(losses, label=opt_name)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test loss\n",
    "    plt.subplot(2, 3, 2)\n",
    "    for opt_name, losses in results['test_loss'].items():\n",
    "        plt.plot(losses, label=opt_name)\n",
    "    plt.title('Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    plt.subplot(2, 3, 3)\n",
    "    for opt_name, accs in results['test_acc'].items():\n",
    "        plt.plot(accs, label=opt_name)\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot time per epoch\n",
    "    plt.subplot(2, 3, 4)\n",
    "    for opt_name, times in results['time_per_epoch'].items():\n",
    "        plt.plot(times, label=opt_name)\n",
    "    plt.title('Time per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot parameter update norm\n",
    "    plt.subplot(2, 3, 5)\n",
    "    for opt_name, norms in results['param_update_norm'].items():\n",
    "        plt.plot(norms, label=opt_name)\n",
    "    plt.title('Parameter Update Norm')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot gradient norm\n",
    "    plt.subplot(2, 3, 6)\n",
    "    for opt_name, norms in results['grad_norm'].items():\n",
    "        plt.plot(norms, label=opt_name)\n",
    "    plt.title('Gradient Norm')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./test/optimizer_benchmark_{dataset_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Define model architectures for testing\n",
    "def create_mlp(input_channels, num_classes):\n",
    "    \"\"\"Create a simple MLP model\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(input_channels * 28 * 28 if input_channels == 1 else input_channels * 32 * 32, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_cnn(input_channels, num_classes):\n",
    "    \"\"\"Create a simple CNN model\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * 7 * 7 if input_channels == 1 else 64 * 8 * 8, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a more challenging convnet model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calculate size after convolutions and pooling\n",
    "        if input_channels == 1:  # MNIST: 28x28\n",
    "            fc_size = 128 * 3 * 3\n",
    "        else:  # CIFAR10: 32x32\n",
    "            fc_size = 128 * 4 * 4\n",
    "            \n",
    "        self.fc1 = nn.Linear(fc_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test all optimizers with different learning rates to find optimal settings\n",
    "def learning_rate_search():\n",
    "    \"\"\"Test different learning rates for each optimizer\"\"\"\n",
    "    test_lrs = {\n",
    "        'SGD': [0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "        'Adam': [0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "        'AdamW': [0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "        'MaxFactor': [0.05, 0.01, 0.005, 0.001, 0.0005]\n",
    "    }\n",
    "    \n",
    "    # Results storage\n",
    "    best_lrs = {}\n",
    "    best_accuracies = {}\n",
    "    \n",
    "    for opt_name, lrs in test_lrs.items():\n",
    "        print(f\"\\nTesting learning rates for {opt_name}\")\n",
    "        best_acc = 0\n",
    "        best_lr = None\n",
    "        \n",
    "        for lr in lrs:\n",
    "            print(f\"  Testing lr={lr}\")\n",
    "            \n",
    "            # Define optimizer constructor with current learning rate\n",
    "            if opt_name == 'SGD':\n",
    "                opt_constructor = lambda params, _: torch.optim.SGD(params, lr=lr, momentum=0.9)\n",
    "            elif opt_name == 'Adam':\n",
    "                opt_constructor = lambda params, _: torch.optim.Adam(params, lr=lr)\n",
    "            elif opt_name == 'AdamW':\n",
    "                opt_constructor = lambda params, _: torch.optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
    "            elif opt_name == 'MaxFactor':\n",
    "                opt_constructor = lambda params, _: MaxFactor(\n",
    "                    params, lr=lr)\n",
    "            \n",
    "            # Test with this optimizer only\n",
    "            optimizers_dict = {opt_name: opt_constructor}\n",
    "            \n",
    "            # Create learning rate dict\n",
    "            lr_dict = {opt_name: lr}\n",
    "            \n",
    "            # Run shorter benchmark\n",
    "            results = optimizer_benchmark(\n",
    "                create_cnn, 'mnist', batch_size=128, epochs=5, \n",
    "                learning_rates=lr_dict, seeds=[42], \n",
    "                optimizers_dict=optimizers_dict, save_plots=False\n",
    "            )\n",
    "            \n",
    "            # Check final accuracy\n",
    "            final_acc = results['test_acc'][opt_name][-1]\n",
    "            if final_acc > best_acc:\n",
    "                best_acc = final_acc\n",
    "                best_lr = lr\n",
    "        \n",
    "        best_lrs[opt_name] = best_lr\n",
    "        best_accuracies[opt_name] = best_acc\n",
    "        print(f\"Best learning rate for {opt_name}: {best_lr} (Accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    return best_lrs, best_accuracies\n",
    "\n",
    "\n",
    "def advanced_test():\n",
    "    \"\"\"Run comprehensive benchmarks\"\"\"\n",
    "    # First find optimal learning rates\n",
    "    best_lrs, _ = learning_rate_search()\n",
    "    \n",
    "    # Define optimizers with best learning rates\n",
    "    optimizers_dict = {\n",
    "        'SGD': lambda params, lr: torch.optim.SGD(params, lr=best_lrs['SGD'], momentum=0.9),\n",
    "        'Adam': lambda params, lr: torch.optim.Adam(params, lr=best_lrs['Adam']),\n",
    "        'AdamW': lambda params, lr: torch.optim.AdamW(params, lr=best_lrs['AdamW'], weight_decay=0.01),\n",
    "        'MaxFactor': lambda params, lr: MaxFactor(params, lr=best_lrs['MaxFactor']\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Run benchmarks on different models and datasets\n",
    "    print(\"\\nRunning comprehensive benchmarks...\")\n",
    "    \n",
    "    # CNN on MNIST\n",
    "    print(\"\\nBenchmarking CNN on MNIST\")\n",
    "    cnn_mnist_results = optimizer_benchmark(\n",
    "        create_cnn, 'mnist', batch_size=128, epochs=10, \n",
    "        learning_rates=best_lrs, seeds=[42, 123, 456], \n",
    "        optimizers_dict=optimizers_dict, save_plots=True\n",
    "    )\n",
    "    \n",
    "    # CNN on CIFAR10\n",
    "    print(\"\\nBenchmarking CNN on CIFAR10\")\n",
    "    cnn_cifar_results = optimizer_benchmark(\n",
    "        create_cnn, 'cifar10', batch_size=128, epochs=10, \n",
    "        learning_rates=best_lrs, seeds=[42, 123, 456], \n",
    "        optimizers_dict=optimizers_dict, save_plots=True\n",
    "    )\n",
    "    \n",
    "    # ConvNet on CIFAR10\n",
    "    print(\"\\nBenchmarking ConvNet on CIFAR10\")\n",
    "    convnet_cifar_results = optimizer_benchmark(\n",
    "        ConvNet, 'cifar10', batch_size=128, epochs=10, \n",
    "        learning_rates=best_lrs, seeds=[42, 123, 456], \n",
    "        optimizers_dict=optimizers_dict, save_plots=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'cnn_mnist': cnn_mnist_results,\n",
    "        'cnn_cifar': cnn_cifar_results,\n",
    "        'convnet_cifar': convnet_cifar_results\n",
    "    }\n",
    "    \n",
    "def memory_usage_test():\n",
    "    \"\"\"Compare memory usage of different optimizers\"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create feature dimensions to test\n",
    "    feature_dims = [100, 200, 400, 800, 1600]\n",
    "    optimizers = ['SGD', 'Adam', 'AdamW', 'MaxFactor']\n",
    "    \n",
    "    # Dictionary to store absolute memory usage (in MB)\n",
    "    memory_usage = {opt: [] for opt in optimizers}\n",
    "    feature_dims_list = []\n",
    "    \n",
    "    # Function to measure GPU memory\n",
    "    def get_gpu_memory_usage():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Fixed model architecture\n",
    "    hidden_dim = 2048  # Large enough to see memory differences\n",
    "    batch_size = 128   # Fixed batch size\n",
    "    \n",
    "    print(\"Measuring memory usage...\")\n",
    "    \n",
    "    # For each feature dimension\n",
    "    for feature_dim in feature_dims:\n",
    "        print(f\"\\nTesting with feature dimension: {feature_dim}\")\n",
    "        feature_dims_list.append(feature_dim)\n",
    "        \n",
    "        # Create fake dataset\n",
    "        x = torch.randn(1000, feature_dim, device=device)\n",
    "        y = torch.randint(0, 10, (1000,), device=device)\n",
    "        \n",
    "        # Test each optimizer with a clean slate\n",
    "        for opt_name in optimizers:\n",
    "            print(f\"  Testing {opt_name}...\")\n",
    "            \n",
    "            # Clear any existing models and optimizers\n",
    "            del x, y  # Delete previous dataset\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Recreate dataset\n",
    "            x = torch.randn(1000, feature_dim, device=device)\n",
    "            y = torch.randint(0, 10, (1000,), device=device)\n",
    "            \n",
    "            # Starting memory - measure after dataset creation\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            base_memory = get_gpu_memory_usage()\n",
    "            \n",
    "            # Create model\n",
    "            model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(feature_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim // 2, 10)\n",
    "            ).to(device)\n",
    "            \n",
    "            # Create optimizer\n",
    "            if opt_name == 'SGD':\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "            elif opt_name == 'Adam':\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "            elif opt_name == 'AdamW':\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "            elif opt_name == 'MaxFactor':\n",
    "                optimizer = MaxFactor(params=model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "            \n",
    "            # Do a single batch update to initialize all optimizer states\n",
    "            inputs, targets = x[:batch_size], y[:batch_size]\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Ensure all operations are complete\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Measure memory after optimizer step\n",
    "            current_memory = get_gpu_memory_usage()\n",
    "            \n",
    "            # Calculate absolute memory used\n",
    "            memory_used = current_memory - base_memory\n",
    "            memory_usage[opt_name].append(memory_used)\n",
    "            \n",
    "            print(f\"    Memory used: {memory_used:.2f} MB\")\n",
    "            \n",
    "            # Clean up\n",
    "            del model, optimizer, inputs, targets, outputs, loss\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'feature_dims': feature_dims_list,\n",
    "        'memory_usage': memory_usage\n",
    "    }\n",
    "    plot_memory_usage(memory_data=result)\n",
    "    return result\n",
    "\n",
    "def plot_memory_usage(memory_data):\n",
    "    \"\"\"Plot memory usage comparison\"\"\"\n",
    "    feature_dims = memory_data['feature_dims']\n",
    "    memory_usage = memory_data['memory_usage']\n",
    "    optimizers = list(memory_usage.keys())\n",
    "    \n",
    "    # Plot memory usage\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    width = 0.2\n",
    "    x = np.arange(len(feature_dims))\n",
    "    \n",
    "    for i, opt_name in enumerate(optimizers):\n",
    "        plt.bar(x + i*width, memory_usage[opt_name], width, label=opt_name)\n",
    "    \n",
    "    plt.xlabel('Feature Dimension')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Memory Usage by Optimizer')\n",
    "    plt.xticks(x + width*1.5, feature_dims)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./test/memory_usage_comparison.png')\n",
    "    \n",
    "    # Return the data\n",
    "    return {\n",
    "        'feature_dims': feature_dims,\n",
    "        'memory_usage': memory_usage\n",
    "    }\n",
    "\n",
    "# Simple test harness\n",
    "def simple_test():\n",
    "    \"\"\"Run a simpler test for quick validation\"\"\"\n",
    "    # Use fixed learning rates\n",
    "    learning_rates = {\n",
    "        'SGD': 0.01,\n",
    "        'Adam': 0.001,\n",
    "        'MaxFactor': 0.01\n",
    "    }\n",
    "    \n",
    "    # Define optimizers\n",
    "    optimizers_dict = {\n",
    "        'SGD': lambda params, lr: torch.optim.SGD(params, lr=learning_rates['SGD'], momentum=0.9),\n",
    "        'Adam': lambda params, lr: torch.optim.Adam(params, lr=learning_rates['Adam']),\n",
    "        'MaxFactor': lambda params, lr: MaxFactor(\n",
    "            params, lr=learning_rates['MaxFactor']\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Run short benchmark on MNIST\n",
    "    results = optimizer_benchmark(\n",
    "        create_cnn, 'mnist', batch_size=128, epochs=5, \n",
    "        learning_rates=learning_rates, seeds=[42], \n",
    "        optimizers_dict=optimizers_dict, save_plots=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def add_memory_usage_to_report(report, memory_data):\n",
    "    \"\"\"Add memory usage information to the benchmark report\"\"\"\n",
    "    report += \"\\nMemory Usage Comparison\\n\"\n",
    "    report += \"=====================\\n\\n\"\n",
    "    \n",
    "    feature_dims = memory_data['feature_dims']\n",
    "    memory_usage = memory_data['memory_usage']\n",
    "    \n",
    "    for dim in range(len(feature_dims)):\n",
    "        report += f\"Feature Dimension: {feature_dims[dim]}\\n\"\n",
    "        report += \"--------------------------\\n\"\n",
    "        for opt_name in memory_usage:\n",
    "            report += f\"  {opt_name}: {memory_usage[opt_name][dim]:.2f} MB\\n\"\n",
    "        report += \"\\n\"\n",
    "    \n",
    "    # Calculate average memory savings\n",
    "    if 'MaxFactor' in memory_usage and 'AdamW' in memory_usage:\n",
    "        avg_maxfactor = np.mean(memory_usage['MaxFactor'])\n",
    "        avg_adamw = np.mean(memory_usage['AdamW'])\n",
    "        savings_pct = 100 * (avg_adamw - avg_maxfactor) / avg_adamw\n",
    "        report += f\"MaxFactor uses {savings_pct:.1f}% less memory than AdamW on average.\\n\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "def summary_report(results):\n",
    "    \"\"\"Generate a summary report of benchmark results\"\"\"\n",
    "    report = \"Optimizer Benchmark Summary\\n\"\n",
    "    report += \"=========================\\n\\n\"\n",
    "    \n",
    "    for dataset, dataset_results in results.items():\n",
    "        report += f\"Dataset: {dataset}\\n\"\n",
    "        report += \"-----------------\\n\"\n",
    "        \n",
    "        # Final test accuracy\n",
    "        report += \"Final Test Accuracy:\\n\"\n",
    "        for opt_name, accs in dataset_results['test_acc'].items():\n",
    "            final_acc = accs[-1]\n",
    "            report += f\"  {opt_name}: {final_acc:.2f}%\\n\"\n",
    "        \n",
    "        # Convergence speed (epochs to reach 90% of final accuracy)\n",
    "        report += \"\\nConvergence Speed (epochs to 90% of final accuracy):\\n\"\n",
    "        for opt_name, accs in dataset_results['test_acc'].items():\n",
    "            final_acc = accs[-1]\n",
    "            target_acc = 0.9 * final_acc\n",
    "            epochs_to_target = next((i for i, acc in enumerate(accs) if acc >= target_acc), len(accs))\n",
    "            report += f\"  {opt_name}: {epochs_to_target} epochs\\n\"\n",
    "        \n",
    "        # Average time per epoch\n",
    "        report += \"\\nAverage Time per Epoch:\\n\"\n",
    "        for opt_name, times in dataset_results['time_per_epoch'].items():\n",
    "            avg_time = np.mean(times)\n",
    "            report += f\"  {opt_name}: {avg_time:.2f}s\\n\"\n",
    "        \n",
    "        # Parameter update statistics\n",
    "        report += \"\\nAverage Parameter Update Norm:\\n\"\n",
    "        for opt_name, norms in dataset_results['param_update_norm'].items():\n",
    "            avg_norm = np.mean(norms)\n",
    "            report += f\"  {opt_name}: {avg_norm:.4f}\\n\"\n",
    "        \n",
    "        report += \"\\n\\n\"\n",
    "    \n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # For quick testing, run simple_test()\n",
    "#     print(\"Running simple test...\")\n",
    "#     simple_results = simple_test()\n",
    "    \n",
    "#     # For comprehensive benchmarks, uncomment the following:\n",
    "#     print(\"Running comprehensive benchmarks...\")\n",
    "#     advanced_results = advanced_test()\n",
    "#     report = summary_report(advanced_results)\n",
    "#     print(report)\n",
    "    \n",
    "\n",
    "#     log_dir = os.path.join('./test/benchmark/', datetime.now().strftime(format='%m-%d_%H'))\n",
    "#     os.makedirs(name=log_dir, exist_ok=True)\n",
    "#     # Save report to file\n",
    "#     with open(log_dir+\"/optimizer_benchmark_report.txt\", \"w\") as f:\n",
    "#         f.write(report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For quick testing, run simple_test()\n",
    "    print(\"Running simple test...\")\n",
    "    simple_results = simple_test()\n",
    "    \n",
    "    # Run memory usage test\n",
    "    print(\"\\nRunning memory usage test...\")\n",
    "    memory_data = memory_usage_test()\n",
    "    \n",
    "    # For comprehensive benchmarks, uncomment the following:\n",
    "    print(\"\\nRunning comprehensive benchmarks...\")\n",
    "    advanced_results = advanced_test()\n",
    "    \n",
    "    # Generate report\n",
    "    report = summary_report(advanced_results)\n",
    "    \n",
    "\n",
    "\n",
    "    report = add_memory_usage_to_report(report, memory_data)\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    log_dir = os.path.join('./test/benchmark/', datetime.now().strftime(format='%m-%d_%H'))\n",
    "    os.makedirs(name=log_dir, exist_ok=True)\n",
    "    # Save report to file\n",
    "    with open(log_dir+\"/benchmarks.txt\", \"w\") as f:\n",
    "        f.write(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
